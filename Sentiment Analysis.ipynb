{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b746c1bb",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1764df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fc7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Provides functions for interacting with the operating system\n",
    "import pandas as pd  # Offers data structures and data analysis tools for Python\n",
    "import numpy as np  # Provides support for large, multi-dimensional arrays and matrices\n",
    "import re  # Supplies regular expression matching operations\n",
    "from collections import Counter  # Implements a counter for counting hashable objects\n",
    "from nltk.tokenize import word_tokenize  # Tokenizes text into words\n",
    "from nltk.corpus import stopwords  # Contains a list of common stop words in various languages\n",
    "from gensim.models import Word2Vec  # Implements the Word2Vec algorithm for word embeddings\n",
    "from arabert.preprocess import ArabertPreprocessor  # Preprocesses Arabic text for NLP tasks\n",
    "from gensim.models import KeyedVectors  # Loads and works with pre-trained word vectors\n",
    "import matplotlib.pyplot as plt  # Provides a MATLAB-like plotting framework\n",
    "from sklearn.model_selection import train_test_split  # Splits arrays or matrices into random train and test subsets\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Converts a collection of text documents to a matrix of token counts\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Computes the cosine similarity between samples\n",
    "from sklearn.manifold import TSNE  # Provides tools for dimensionality reduction\n",
    "from transformers import AutoTokenizer, AutoModel  # Offers pre-trained models and tokenizers for NLP tasks\n",
    "import torch  # Provides an optimized tensor library for deep learning\n",
    "from sklearn.preprocessing import LabelEncoder  # Encodes target labels with value between 0 and n_classes-1\n",
    "from sklearn.neural_network import MLPClassifier  # Implements a multi-layer perceptron (MLP) algorithm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # Provides metrics to evaluate classification performance\n",
    "from sklearn.metrics import classification_report  # Generates a text report showing main classification metrics\n",
    "from keras.models import Sequential  # Allows creating a sequential neural network model\n",
    "from keras.layers import LSTM, Dense, Dropout  # Provides layers for LSTM networks, dense layers, and dropout regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce15997",
   "metadata": {},
   "source": [
    "# Load and Combine the Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908018d",
   "metadata": {},
   "source": [
    "1- Load the provided TSV files.\n",
    "\n",
    "2- Combine positive and negative samples.\n",
    "\n",
    "3- Split the training data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b398a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(data_dir, num_samples=2500):\n",
    "    \n",
    "    # Define directories for negative and positive data\n",
    "    neg_dir = os.path.join(data_dir, 'neg')\n",
    "    pos_dir = os.path.join(data_dir, 'pos')\n",
    "    \n",
    "    # Get filenames for negative and positive samples\n",
    "    neg_files = os.listdir(neg_dir)[:num_samples//2]\n",
    "    pos_files = os.listdir(pos_dir)[:num_samples//2]\n",
    "    \n",
    "    # Read data from files\n",
    "    neg_data = [open(os.path.join(neg_dir, file), 'r', encoding='utf-8').read() for file in neg_files]\n",
    "    pos_data = [open(os.path.join(pos_dir, file), 'r', encoding='utf-8').read() for file in pos_files]\n",
    "    \n",
    "    # Create DataFrames for negative and positive data\n",
    "    df_neg = pd.DataFrame({'text': neg_data, 'sentiment': ['negative']*len(neg_data)})\n",
    "    df_pos = pd.DataFrame({'text': pos_data, 'sentiment': ['positive']*len(pos_data)})\n",
    "    \n",
    "    # Concatenate and reset the index of DataFrames\n",
    "    return pd.concat([df_neg, df_pos]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2b51f",
   "metadata": {},
   "source": [
    "# Step 2: Tokenize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13fd87",
   "metadata": {},
   "source": [
    "Step 2.1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7be952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f11a3",
   "metadata": {},
   "source": [
    " Step 2.2: Remove of Punctuation, Diacritical Marks(التشكيل), Numbers, and Non-Arabic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f419d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    arabic_tokens = [word for word in tokens if re.match(r'^[\\u0600-\\u06FF]+$', word)]\n",
    "    cleaned_tokens = [re.sub(arabic_punctuation, '', word) for word in arabic_tokens]\n",
    "    cleaned_tokens = [re.sub(arabic_diacritics, '', word) for word in cleaned_tokens]\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62c84f",
   "metadata": {},
   "source": [
    "Step 2.3: Data representation using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4081ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_data_with_bow(data):\n",
    "    # Initialize CountVectorizer with custom tokenizer and preprocessor that do nothing\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x)\n",
    "    \n",
    "    # Fit and transform the 'tokens' column of the input data to a sparse matrix\n",
    "    X = vectorizer.fit_transform(data['tokens'])\n",
    "    \n",
    "    # Convert the sparse matrix to a dense DataFrame with token names as columns\n",
    "    bow_representation = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Return the BoW DataFrame and the fitted vectorizer\n",
    "    return bow_representation, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee134f9",
   "metadata": {},
   "source": [
    "Step 2.4: Data representation function using arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a5e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_data_with_arabert(data, tokenizer, model):\n",
    "    embeddings = []  # List to store sentence embeddings\n",
    "    all_words = []  # List to store sentences\n",
    "\n",
    "    for w_tokens in data['tokens']:\n",
    "        sentence = ' '.join(w_tokens)  # Join tokens to form a sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)  # Tokenize the sentence\n",
    "        outputs = model(**inputs)  # Get model outputs\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()  # Compute mean embedding\n",
    "        embeddings.append(embedding)  # Append embedding to list\n",
    "        all_words.append(sentence)  # Append sentence to list\n",
    "    \n",
    "    return embeddings, all_words  # Return embeddings and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b19ab",
   "metadata": {},
   "source": [
    "Step 2.5: Print the most similar words using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a67110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_words_bow(word, bow_vectorizer, bow_representation):\n",
    "    \n",
    "    # Check if the word is in the BoW vocabulary\n",
    "    if word not in bow_vectorizer.get_feature_names_out():\n",
    "        print(f\"'{word}' not found in the BoW vocabulary.\")\n",
    "        return\n",
    "    \n",
    "    # Get the index of the word in the vocabulary\n",
    "    word_index = bow_vectorizer.vocabulary_[word]\n",
    "    \n",
    "    # Get the word's vector from the BoW representation\n",
    "    word_vector = bow_representation.iloc[:, word_index].values.reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarities between the word vector and all other vectors\n",
    "    similarities = cosine_similarity(word_vector, bow_representation.T)\n",
    "    \n",
    "    # Get indices of the top 5 most similar words (excluding the word itself)\n",
    "    most_similar_indices = similarities.argsort()[0][-6:-1]\n",
    "    \n",
    "    # Get the most similar words using the indices\n",
    "    most_similar_words = [bow_vectorizer.get_feature_names_out()[idx] for idx in most_similar_indices]\n",
    "    \n",
    "    # Print the most similar words\n",
    "    print(f\"Most similar words to '{word}' using BoW:\")\n",
    "    for w in most_similar_words:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46332778",
   "metadata": {},
   "source": [
    "Step 2.6: Print the most similar words using AraBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78dccb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_words(word, embeddings, all_words, tokenizer, model):\n",
    "    try:\n",
    "        # Tokenize the word and get its embedding\n",
    "        inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "        \n",
    "        # Convert embeddings list to numpy array\n",
    "        embeddings_array = np.array(embeddings)\n",
    "        \n",
    "        # Calculate cosine similarities between the word embedding and all embeddings\n",
    "        similarities = cosine_similarity(word_embedding.reshape(1, -1), embeddings_array)\n",
    "        \n",
    "        # Get indices of the top 5 most similar words (excluding the word itself)\n",
    "        most_similar_indices = similarities.argsort()[0][-6:-1]\n",
    "        \n",
    "        # Get the most similar words using the indices\n",
    "        most_similar_words = [all_words[idx] for idx in most_similar_indices]\n",
    "        \n",
    "        # Print the most similar words\n",
    "        print(f\"Most similar words to '{word}':\")\n",
    "        for w in most_similar_words:\n",
    "            print(w)\n",
    "    except KeyError:\n",
    "        # Handle the case where the word is not found in the vocabulary\n",
    "        print(f\"'{word}' not found in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb85be",
   "metadata": {},
   "source": [
    "Step 2.7: Plot and visualize the embedding and their similarities using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf2e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings_bow(words, vectorizer):\n",
    "    \n",
    "    word_vectors = []\n",
    "    \n",
    "    # Iterate over each word\n",
    "    for word in words:\n",
    "        # Check if the word is in the BoW vocabulary\n",
    "        if word in vectorizer.get_feature_names_out():\n",
    "            # If found, transform the word to its vector representation and append to the list\n",
    "            word_vectors.append(vectorizer.transform([word]).toarray().flatten())\n",
    "    \n",
    "    # Check if any word vectors were found\n",
    "    if not word_vectors:\n",
    "        print(\"No words found in the BoW vocabulary for visualization.\")\n",
    "        return\n",
    "\n",
    "    # Convert the list of word vectors to a numpy array\n",
    "    vectors = np.array(word_vectors)\n",
    "    \n",
    "    # Initialize t-SNE model with specific parameters\n",
    "    tsne_model = TSNE(perplexity=2, n_components=2, init='pca', n_iter=2500, random_state=23)  # Further reduce perplexity\n",
    "    \n",
    "    # Fit and transform the word vectors using t-SNE\n",
    "    new_values = tsne_model.fit_transform(vectors)\n",
    "\n",
    "    # Extract x and y coordinates from the transformed values\n",
    "    x = [value[0] for value in new_values]\n",
    "    y = [value[1] for value in new_values]\n",
    "\n",
    "    # Plot the scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        # Annotate each point with its corresponding word\n",
    "        plt.annotate(words[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9684acd",
   "metadata": {},
   "source": [
    "Step 2.8: Plot and visualize the embedding and their similarities using AraBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fdda1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(words, embeddings, tokenizer, model):\n",
    "    word_embeddings = []\n",
    "    \n",
    "    # Iterate over each word\n",
    "    for word in words:\n",
    "        # Tokenize the word and get its embedding\n",
    "        inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "        word_embeddings.append(word_embedding)\n",
    "    \n",
    "    # Convert the list of word embeddings to a numpy array\n",
    "    vectors = np.array(word_embeddings)\n",
    "    \n",
    "    # Initialize t-SNE model with specific parameters\n",
    "    tsne_model = TSNE(perplexity=2, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    \n",
    "    # Fit and transform the word embeddings using t-SNE\n",
    "    new_values = tsne_model.fit_transform(vectors)\n",
    "\n",
    "    # Extract x and y coordinates from the transformed values\n",
    "    x = [value[0] for value in new_values]\n",
    "    y = [value[1] for value in new_values]\n",
    "\n",
    "    # Plot the scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        # Annotate each point with its corresponding word\n",
    "        plt.annotate(words[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cc321",
   "metadata": {},
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf7e945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو بيدي أرجع ساعتي وين أرجع؟ إليا صدفه؟ والله ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>💥 حمدا لله .. القيادة كما هي والاعداد في تزايد...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#الهلال_الاهلي مبروك للهلال هاردلك للطحالب 🐸 و...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>أحب نفسك، أعشقها ودللها .. فتش عن السعادة ستجد...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>#وش_يقول_الليل يقول ارقدي وفكي المسلمين شرك 🌚 ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>اللهم انك ترى ما لا نرى وتعلم ما لا نعلم فاكفن...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[ ☆ ] : ون ذا ناين من الفانميتنق .\\n</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment\n",
       "0     اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...  negative\n",
       "1     توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...  negative\n",
       "2     لو بيدي أرجع ساعتي وين أرجع؟ إليا صدفه؟ والله ...  negative\n",
       "3     💥 حمدا لله .. القيادة كما هي والاعداد في تزايد...  negative\n",
       "4     #الهلال_الاهلي مبروك للهلال هاردلك للطحالب 🐸 و...  negative\n",
       "...                                                 ...       ...\n",
       "4995  أحب نفسك، أعشقها ودللها .. فتش عن السعادة ستجد...  positive\n",
       "4996  بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...  positive\n",
       "4997  #وش_يقول_الليل يقول ارقدي وفكي المسلمين شرك 🌚 ...  positive\n",
       "4998  اللهم انك ترى ما لا نرى وتعلم ما لا نعلم فاكفن...  positive\n",
       "4999               [ ☆ ] : ون ذا ناين من الفانميتنق .\\n  positive\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r'C:\\Users\\DELL\\Desktop\\Specialist\\DataSet\\Sentiment Analysis\\data\\arabic_tweets'\n",
    "data = read_dataset(data_dir, num_samples=5000)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4b82d",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bb7033bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)  # This line downloads the default tokenizer (English)\n",
    "\n",
    "# Now, download the Arabic tokenization model\n",
    "nltk.download('punkt', quiet=True, raise_on_error=True, halt_on_error=False, download_dir=r'~/nltk_data/tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacffffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[اعترف, ان, بتس, كانو, شوي, شوي, يجيبو, راسي, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[توقعت, اذا, جات, داريا, بشوفهم, كاملين, بس, ل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو بيدي أرجع ساعتي وين أرجع؟ إليا صدفه؟ والله ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[لو, بيدي, أرجع, ساعتي, وين, أرجع؟, إليا, صدفه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>💥 حمدا لله .. القيادة كما هي والاعداد في تزايد...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[💥, حمدا, لله, .., القيادة, كما, هي, والاعداد,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#الهلال_الاهلي مبروك للهلال هاردلك للطحالب 🐸 و...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[#, الهلال_الاهلي, مبروك, للهلال, هاردلك, للطح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>أحب نفسك، أعشقها ودللها .. فتش عن السعادة ستجد...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[أحب, نفسك،, أعشقها, ودللها, .., فتش, عن, السع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[بمناسبة, فوز, الهلال, .., 💙, سحب, على, آيفون,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>#وش_يقول_الليل يقول ارقدي وفكي المسلمين شرك 🌚 ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[#, وش_يقول_الليل, يقول, ارقدي, وفكي, المسلمين...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>اللهم انك ترى ما لا نرى وتعلم ما لا نعلم فاكفن...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[اللهم, انك, ترى, ما, لا, نرى, وتعلم, ما, لا, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[ ☆ ] : ون ذا ناين من الفانميتنق .\\n</td>\n",
       "      <td>positive</td>\n",
       "      <td>[[, ☆, ], :, ون, ذا, ناين, من, الفانميتنق, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...  negative   \n",
       "1     توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...  negative   \n",
       "2     لو بيدي أرجع ساعتي وين أرجع؟ إليا صدفه؟ والله ...  negative   \n",
       "3     💥 حمدا لله .. القيادة كما هي والاعداد في تزايد...  negative   \n",
       "4     #الهلال_الاهلي مبروك للهلال هاردلك للطحالب 🐸 و...  negative   \n",
       "...                                                 ...       ...   \n",
       "4995  أحب نفسك، أعشقها ودللها .. فتش عن السعادة ستجد...  positive   \n",
       "4996  بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...  positive   \n",
       "4997  #وش_يقول_الليل يقول ارقدي وفكي المسلمين شرك 🌚 ...  positive   \n",
       "4998  اللهم انك ترى ما لا نرى وتعلم ما لا نعلم فاكفن...  positive   \n",
       "4999               [ ☆ ] : ون ذا ناين من الفانميتنق .\\n  positive   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [اعترف, ان, بتس, كانو, شوي, شوي, يجيبو, راسي, ...  \n",
       "1     [توقعت, اذا, جات, داريا, بشوفهم, كاملين, بس, ل...  \n",
       "2     [لو, بيدي, أرجع, ساعتي, وين, أرجع؟, إليا, صدفه...  \n",
       "3     [💥, حمدا, لله, .., القيادة, كما, هي, والاعداد,...  \n",
       "4     [#, الهلال_الاهلي, مبروك, للهلال, هاردلك, للطح...  \n",
       "...                                                 ...  \n",
       "4995  [أحب, نفسك،, أعشقها, ودللها, .., فتش, عن, السع...  \n",
       "4996  [بمناسبة, فوز, الهلال, .., 💙, سحب, على, آيفون,...  \n",
       "4997  [#, وش_يقول_الليل, يقول, ارقدي, وفكي, المسلمين...  \n",
       "4998  [اللهم, انك, ترى, ما, لا, نرى, وتعلم, ما, لا, ...  \n",
       "4999      [[, ☆, ], :, ون, ذا, ناين, من, الفانميتنق, .]  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['text'].apply(tokenize_text)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9b5b9",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2f8e7",
   "metadata": {},
   "source": [
    "Arabic punctuation marks and diacritical marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33fc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_punctuation = r'[\\u060C\\u061B\\u061F\\u066A\\u066B\\u066C\\u066D\\u06D4\\u06DD\\u06DE\\u06E9\\u06EA\\u06EB\\u06EC\\u06ED\\uFD3E\\uFD3F]'\n",
    "arabic_diacritics = r'[\\u0610-\\u061A\\u064B-\\u065F\\u06D6-\\u06ED]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29411e99",
   "metadata": {},
   "source": [
    "Removal of Punctuation, Diacritical Marks, Numbers, and Non-Arabic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e4dfdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[اعترف, ان, بتس, كانو, شوي, شوي, يجيبو, راسي, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[توقعت, اذا, جات, داريا, بشوفهم, كاملين, بس, ل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لو بيدي أرجع ساعتي وين أرجع؟ إليا صدفه؟ والله ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[لو, بيدي, أرجع, ساعتي, وين, أرجع, إليا, صدفه,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>💥 حمدا لله .. القيادة كما هي والاعداد في تزايد...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[حمدا, لله, القيادة, كما, هي, والاعداد, في, تز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#الهلال_الاهلي مبروك للهلال هاردلك للطحالب 🐸 و...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[مبروك, للهلال, هاردلك, للطحالب, وطز, في, النصر]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>أحب نفسك، أعشقها ودللها .. فتش عن السعادة ستجد...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[أحب, نفسك, أعشقها, ودللها, فتش, عن, السعادة, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[بمناسبة, فوز, الهلال, سحب, على, آيفون, رتويت,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>#وش_يقول_الليل يقول ارقدي وفكي المسلمين شرك 🌚 ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[يقول, ارقدي, وفكي, المسلمين, شرك, بس, انا, قل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>اللهم انك ترى ما لا نرى وتعلم ما لا نعلم فاكفن...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[اللهم, انك, ترى, ما, لا, نرى, وتعلم, ما, لا, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[ ☆ ] : ون ذا ناين من الفانميتنق .\\n</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ون, ذا, ناين, من, الفانميتنق]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...  negative   \n",
       "1     توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...  negative   \n",
       "2     لو بيدي أرجع ساعتي وين أرجع؟ إليا صدفه؟ والله ...  negative   \n",
       "3     💥 حمدا لله .. القيادة كما هي والاعداد في تزايد...  negative   \n",
       "4     #الهلال_الاهلي مبروك للهلال هاردلك للطحالب 🐸 و...  negative   \n",
       "...                                                 ...       ...   \n",
       "4995  أحب نفسك، أعشقها ودللها .. فتش عن السعادة ستجد...  positive   \n",
       "4996  بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...  positive   \n",
       "4997  #وش_يقول_الليل يقول ارقدي وفكي المسلمين شرك 🌚 ...  positive   \n",
       "4998  اللهم انك ترى ما لا نرى وتعلم ما لا نعلم فاكفن...  positive   \n",
       "4999               [ ☆ ] : ون ذا ناين من الفانميتنق .\\n  positive   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [اعترف, ان, بتس, كانو, شوي, شوي, يجيبو, راسي, ...  \n",
       "1     [توقعت, اذا, جات, داريا, بشوفهم, كاملين, بس, ل...  \n",
       "2     [لو, بيدي, أرجع, ساعتي, وين, أرجع, إليا, صدفه,...  \n",
       "3     [حمدا, لله, القيادة, كما, هي, والاعداد, في, تز...  \n",
       "4      [مبروك, للهلال, هاردلك, للطحالب, وطز, في, النصر]  \n",
       "...                                                 ...  \n",
       "4995  [أحب, نفسك, أعشقها, ودللها, فتش, عن, السعادة, ...  \n",
       "4996  [بمناسبة, فوز, الهلال, سحب, على, آيفون, رتويت,...  \n",
       "4997  [يقول, ارقدي, وفكي, المسلمين, شرك, بس, انا, قل...  \n",
       "4998  [اللهم, انك, ترى, ما, لا, نرى, وتعلم, ما, لا, ...  \n",
       "4999                     [ون, ذا, ناين, من, الفانميتنق]  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['text'].apply(lambda x: clean_tokens(tokenize_text(x)))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfcb4e",
   "metadata": {},
   "source": [
    "# Data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79279f61",
   "metadata": {},
   "source": [
    "Data representation using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d4f0199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>؏</th>\n",
       "      <th>؏ليك</th>\n",
       "      <th>ء</th>\n",
       "      <th>آااه</th>\n",
       "      <th>آبائنا</th>\n",
       "      <th>آبدآع</th>\n",
       "      <th>آبيه</th>\n",
       "      <th>آتخيل</th>\n",
       "      <th>آتكبر</th>\n",
       "      <th>...</th>\n",
       "      <th>گمان</th>\n",
       "      <th>ھادئ</th>\n",
       "      <th>ھھ</th>\n",
       "      <th>ۆ</th>\n",
       "      <th>ۆشلون</th>\n",
       "      <th>ۆلكن</th>\n",
       "      <th>ۈ</th>\n",
       "      <th>ۈالله</th>\n",
       "      <th>یبقی</th>\n",
       "      <th>یوم</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 17256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ؏  ؏ليك  ء  آااه  آبائنا  آبدآع  آبيه  آتخيل  آتكبر  ...  گمان  ھادئ  \\\n",
       "0     0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "1     0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "2     0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "3     0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "4     0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "...  .. ..   ... ..   ...     ...    ...   ...    ...    ...  ...   ...   ...   \n",
       "4995  0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "4996  0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "4997  0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "4998  0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "4999  0  0     0  0     0       0      0     0      0      0  ...     0     0   \n",
       "\n",
       "      ھھ  ۆ  ۆشلون  ۆلكن  ۈ  ۈالله  یبقی  یوم  \n",
       "0      0  0      0     0  0      0     0    0  \n",
       "1      0  0      0     0  0      0     0    0  \n",
       "2      0  0      0     0  0      0     0    0  \n",
       "3      0  0      0     0  0      0     0    0  \n",
       "4      0  0      0     0  0      0     0    0  \n",
       "...   .. ..    ...   ... ..    ...   ...  ...  \n",
       "4995   0  0      0     0  0      0     0    0  \n",
       "4996   0  0      0     0  0      0     0    0  \n",
       "4997   0  0      0     0  0      0     0    0  \n",
       "4998   0  0      0     0  0      0     0    0  \n",
       "4999   0  0      0     0  0      0     0    0  \n",
       "\n",
       "[5000 rows x 17256 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_representation, bow_vectorizer = represent_data_with_bow(data)\n",
    "bow_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5dd16a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function represent_data_with_bow.<locals>.<lambda> at 0x0000019794D4AC10>,\n",
       "                tokenizer=<function represent_data_with_bow.<locals>.<lambda> at 0x0000019794D4AF70>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51369e28",
   "metadata": {},
   "source": [
    "Data representation using Arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cbdc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabert\")\n",
    "model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "038ed75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_embeddings, arabert_words = represent_data_with_arabert(data, tokenizer, model)\n",
    "# arabert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75e23423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabert_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ff638",
   "metadata": {},
   "source": [
    "# Print the most similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fbbbc",
   "metadata": {},
   "source": [
    "BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cde4f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'امتحان' using BoW:\n",
      "لتقديركم\n",
      "انم\n",
      "تاى\n",
      "وثب\n",
      "ماكنسلو\n",
      "Most similar words to 'الأهلي' using BoW:\n",
      "طرد\n",
      "لكرة\n",
      "إذن\n",
      "والتغافل\n",
      "إنتظار\n",
      "Most similar words to 'الاتحاد' using BoW:\n",
      "مختلفا\n",
      "مهدد\n",
      "للدوري\n",
      "بطلا\n",
      "يتوج\n"
     ]
    }
   ],
   "source": [
    "words_to_check = [\"امتحان\", \"الأهلي\", \"الاتحاد\"]\n",
    "for word in words_to_check:\n",
    "    print_similar_words_bow(word, bow_vectorizer, bow_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07cf2a",
   "metadata": {},
   "source": [
    "AraBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6595c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'امتحان':\n",
      "شهور تستاهل الزرقاء\n",
      "شهور تستاهل الزرقاء\n",
      "ي حظك\n",
      "صح نومي\n",
      "للاسف دوام\n",
      "Most similar words to 'الأهلي':\n",
      "المطر مشالله\n",
      "خسارة التعليم فيهم\n",
      "صبااح النصر\n",
      "الجو يكسل\n",
      "طفو الكهرب\n",
      "Most similar words to 'الاتحاد':\n",
      "صبااح النصر\n",
      "صباح الاتحاد\n",
      "سماعاتي خربوا\n",
      "تيشرت القائد\n",
      "طفو الكهرب\n"
     ]
    }
   ],
   "source": [
    "words_to_check = [\"امتحان\", \"الأهلي\", \"الاتحاد\"]\n",
    "for word in words_to_check:\n",
    "    print_similar_words(word, arabert_embeddings, arabert_words, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79613435",
   "metadata": {},
   "source": [
    "# Plot and visualize the embedding and their similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30adea8",
   "metadata": {},
   "source": [
    "BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b882e2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot and visualize the embedding and their similarities using BoW\n",
    "visualize_embeddings_bow(words_to_check, bow_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b6e7b",
   "metadata": {},
   "source": [
    "AraBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a13d011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_embeddings(words_to_check, arabert_embeddings, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6033be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets for Bag-of-Words (BoW) representation\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    bow_representation, data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the data into training and testing sets for AraBERT representation\n",
    "X_train_arabert, X_test_arabert, y_train_arabert, y_test_arabert = train_test_split(\n",
    "    arabert_embeddings, data['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a494a4",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efe2c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LabelEncoder\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27432752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow\n",
    "# Fit and transform the class labels in your training set\n",
    "y_train_encoded_bow = label_encoder.fit_transform(y_train_bow)\n",
    "# Transform the class labels in your test set\n",
    "y_test_encoded_bow = label_encoder.transform(y_test_bow)\n",
    "\n",
    "# arabert\n",
    "# Fit and transform the class labels in your training set\n",
    "y_train_encoded_arabert = label_encoder.transform(y_train_arabert)\n",
    "# Transform the class labels in your test set\n",
    "y_test_encoded_arabert = label_encoder.transform(y_test_arabert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e8963",
   "metadata": {},
   "source": [
    "# Build the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032e1f4",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c85ea9",
   "metadata": {},
   "source": [
    "Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f14fdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(4,), random_state=42)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build an artificial neural network\n",
    "ann_model_bow = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', solver='adam', random_state=42)\n",
    "ann_model_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cdb80",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f67749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an LSTM model\n",
    "lstm_model_bow = Sequential([\n",
    "    LSTM(64, input_shape=(1, X_train_bow.shape[1])),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5349b",
   "metadata": {},
   "source": [
    "# AraBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edc282",
   "metadata": {},
   "source": [
    "Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "770bd4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(4,), random_state=42)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model_arabert = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', solver='adam', random_state=42)\n",
    "ann_model_arabert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a7222",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5741db2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 768)\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train_arabert is a list\n",
    "X_train_arabert = np.array(X_train_arabert)  # Convert to NumPy array\n",
    "\n",
    "# Now you can access its shape attribute\n",
    "print(X_train_arabert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27c68eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data for LSTM model\n",
    "X_train_arabert_reshaped = np.expand_dims(X_train_arabert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e350d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model for AraBERT representation\n",
    "lstm_model_arabert = Sequential([\n",
    "    LSTM(64, input_shape=(1, X_train_arabert_reshaped.shape[2])),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "901f3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the LSTM model\n",
    "lstm_model_arabert.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dc4c2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723ebf7",
   "metadata": {},
   "source": [
    "BoW Representation\n",
    "\n",
    "Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a9ab47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(4,), random_state=42)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model_bow.fit(X_train_bow, y_train_encoded_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab724c8",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "647bf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_bow.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fff808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data for LSTM model\n",
    "X_train_bow_reshaped = np.expand_dims(X_train_bow, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b593b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.6006 - loss: 0.6739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x197e9ac0fa0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_bow.fit(X_train_bow_reshaped, y_train_encoded_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d58c7",
   "metadata": {},
   "source": [
    "AraBERT Representation\n",
    "\n",
    "Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53529450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(4,), random_state=42)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model_arabert.fit(X_train_arabert, y_train_encoded_arabert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e5005",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1475481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5628 - loss: 0.6801 - val_accuracy: 0.6538 - val_loss: 0.6405\n",
      "Epoch 2/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6489 - loss: 0.6273 - val_accuracy: 0.6350 - val_loss: 0.6535\n",
      "Epoch 3/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6578 - loss: 0.6193 - val_accuracy: 0.6425 - val_loss: 0.6260\n",
      "Epoch 4/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6825 - loss: 0.6016 - val_accuracy: 0.6800 - val_loss: 0.6129\n",
      "Epoch 5/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6895 - loss: 0.5954 - val_accuracy: 0.6587 - val_loss: 0.6254\n",
      "Epoch 6/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7035 - loss: 0.5656 - val_accuracy: 0.6600 - val_loss: 0.6203\n",
      "Epoch 7/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6929 - loss: 0.5682 - val_accuracy: 0.6662 - val_loss: 0.6133\n",
      "Epoch 8/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7321 - loss: 0.5420 - val_accuracy: 0.6600 - val_loss: 0.6580\n",
      "Epoch 9/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7305 - loss: 0.5338 - val_accuracy: 0.6488 - val_loss: 0.6722\n",
      "Epoch 10/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7380 - loss: 0.5286 - val_accuracy: 0.6712 - val_loss: 0.6335\n",
      "Epoch 11/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7502 - loss: 0.5105 - val_accuracy: 0.6488 - val_loss: 0.6256\n",
      "Epoch 12/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7555 - loss: 0.4932 - val_accuracy: 0.6850 - val_loss: 0.6322\n",
      "Epoch 13/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7638 - loss: 0.4740 - val_accuracy: 0.6687 - val_loss: 0.6340\n",
      "Epoch 14/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7673 - loss: 0.4748 - val_accuracy: 0.6812 - val_loss: 0.6137\n",
      "Epoch 15/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7615 - loss: 0.4714 - val_accuracy: 0.6913 - val_loss: 0.6206\n",
      "Epoch 16/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7908 - loss: 0.4402 - val_accuracy: 0.6925 - val_loss: 0.6424\n",
      "Epoch 17/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7926 - loss: 0.4300 - val_accuracy: 0.6825 - val_loss: 0.6260\n",
      "Epoch 18/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7999 - loss: 0.4169 - val_accuracy: 0.6862 - val_loss: 0.6588\n",
      "Epoch 19/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8204 - loss: 0.3994 - val_accuracy: 0.7000 - val_loss: 0.6354\n",
      "Epoch 20/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8391 - loss: 0.3793 - val_accuracy: 0.6812 - val_loss: 0.6581\n",
      "Epoch 21/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8344 - loss: 0.3686 - val_accuracy: 0.6787 - val_loss: 0.6541\n",
      "Epoch 22/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8296 - loss: 0.3737 - val_accuracy: 0.6950 - val_loss: 0.6704\n",
      "Epoch 23/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8373 - loss: 0.3609 - val_accuracy: 0.6925 - val_loss: 0.6982\n",
      "Epoch 24/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8481 - loss: 0.3378 - val_accuracy: 0.6812 - val_loss: 0.6848\n",
      "Epoch 25/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8480 - loss: 0.3336 - val_accuracy: 0.7000 - val_loss: 0.6755\n",
      "Epoch 26/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8546 - loss: 0.3184 - val_accuracy: 0.6837 - val_loss: 0.7070\n",
      "Epoch 27/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8546 - loss: 0.3263 - val_accuracy: 0.6862 - val_loss: 0.7278\n",
      "Epoch 28/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8656 - loss: 0.3085 - val_accuracy: 0.6862 - val_loss: 0.7453\n",
      "Epoch 29/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8838 - loss: 0.2775 - val_accuracy: 0.6913 - val_loss: 0.7312\n",
      "Epoch 30/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8871 - loss: 0.2641 - val_accuracy: 0.6900 - val_loss: 0.7347\n",
      "Epoch 31/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8838 - loss: 0.2649 - val_accuracy: 0.6975 - val_loss: 0.7530\n",
      "Epoch 32/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9065 - loss: 0.2370 - val_accuracy: 0.7088 - val_loss: 0.7356\n",
      "Epoch 33/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9053 - loss: 0.2384 - val_accuracy: 0.6950 - val_loss: 0.7327\n",
      "Epoch 34/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9134 - loss: 0.2367 - val_accuracy: 0.6925 - val_loss: 0.7613\n",
      "Epoch 35/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9000 - loss: 0.2279 - val_accuracy: 0.7075 - val_loss: 0.7570\n",
      "Epoch 36/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9141 - loss: 0.2260 - val_accuracy: 0.7088 - val_loss: 0.7817\n",
      "Epoch 37/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9228 - loss: 0.2004 - val_accuracy: 0.7113 - val_loss: 0.7863\n",
      "Epoch 38/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9320 - loss: 0.1979 - val_accuracy: 0.7125 - val_loss: 0.8068\n",
      "Epoch 39/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9223 - loss: 0.1966 - val_accuracy: 0.7125 - val_loss: 0.7877\n",
      "Epoch 40/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9301 - loss: 0.1904 - val_accuracy: 0.6938 - val_loss: 0.8690\n",
      "Epoch 41/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9332 - loss: 0.1796 - val_accuracy: 0.7075 - val_loss: 0.8000\n",
      "Epoch 42/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9391 - loss: 0.1692 - val_accuracy: 0.7113 - val_loss: 0.8197\n",
      "Epoch 43/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9400 - loss: 0.1636 - val_accuracy: 0.7150 - val_loss: 0.8335\n",
      "Epoch 44/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9391 - loss: 0.1680 - val_accuracy: 0.7100 - val_loss: 0.8263\n",
      "Epoch 45/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9356 - loss: 0.1701 - val_accuracy: 0.7088 - val_loss: 0.8410\n",
      "Epoch 46/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9619 - loss: 0.1309 - val_accuracy: 0.7050 - val_loss: 0.8792\n",
      "Epoch 47/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9496 - loss: 0.1476 - val_accuracy: 0.7088 - val_loss: 0.8602\n",
      "Epoch 48/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9476 - loss: 0.1427 - val_accuracy: 0.7150 - val_loss: 0.8589\n",
      "Epoch 49/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9603 - loss: 0.1285 - val_accuracy: 0.6963 - val_loss: 0.9691\n",
      "Epoch 50/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9498 - loss: 0.1256 - val_accuracy: 0.7225 - val_loss: 0.9257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9460 - loss: 0.1426 - val_accuracy: 0.7138 - val_loss: 0.9290\n",
      "Epoch 52/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9551 - loss: 0.1212 - val_accuracy: 0.6862 - val_loss: 0.9919\n",
      "Epoch 53/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9491 - loss: 0.1328 - val_accuracy: 0.7013 - val_loss: 0.9805\n",
      "Epoch 54/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9662 - loss: 0.1021 - val_accuracy: 0.7038 - val_loss: 0.9738\n",
      "Epoch 55/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9659 - loss: 0.1061 - val_accuracy: 0.7063 - val_loss: 0.9407\n",
      "Epoch 56/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9611 - loss: 0.1070 - val_accuracy: 0.6988 - val_loss: 0.9636\n",
      "Epoch 57/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9598 - loss: 0.1121 - val_accuracy: 0.7125 - val_loss: 0.9727\n",
      "Epoch 58/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9704 - loss: 0.0879 - val_accuracy: 0.6775 - val_loss: 1.0374\n",
      "Epoch 59/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9651 - loss: 0.0998 - val_accuracy: 0.7175 - val_loss: 0.9914\n",
      "Epoch 60/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9732 - loss: 0.0842 - val_accuracy: 0.7075 - val_loss: 1.0335\n",
      "Epoch 61/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9674 - loss: 0.0962 - val_accuracy: 0.7000 - val_loss: 0.9913\n",
      "Epoch 62/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9732 - loss: 0.0880 - val_accuracy: 0.7063 - val_loss: 1.0234\n",
      "Epoch 63/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9639 - loss: 0.0949 - val_accuracy: 0.7025 - val_loss: 1.0651\n",
      "Epoch 64/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9662 - loss: 0.0930 - val_accuracy: 0.7038 - val_loss: 1.0292\n",
      "Epoch 65/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9691 - loss: 0.0931 - val_accuracy: 0.7038 - val_loss: 1.0659\n",
      "Epoch 66/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9701 - loss: 0.0862 - val_accuracy: 0.6975 - val_loss: 1.1008\n",
      "Epoch 67/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9722 - loss: 0.0820 - val_accuracy: 0.6938 - val_loss: 1.1079\n",
      "Epoch 68/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9681 - loss: 0.0808 - val_accuracy: 0.7038 - val_loss: 1.1075\n",
      "Epoch 69/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9729 - loss: 0.0677 - val_accuracy: 0.6963 - val_loss: 1.0740\n",
      "Epoch 70/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9768 - loss: 0.0738 - val_accuracy: 0.6975 - val_loss: 1.1184\n",
      "Epoch 71/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9760 - loss: 0.0750 - val_accuracy: 0.7000 - val_loss: 1.1584\n",
      "Epoch 72/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9792 - loss: 0.0677 - val_accuracy: 0.7025 - val_loss: 1.1566\n",
      "Epoch 73/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9828 - loss: 0.0593 - val_accuracy: 0.7125 - val_loss: 1.1807\n",
      "Epoch 74/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9782 - loss: 0.0651 - val_accuracy: 0.7025 - val_loss: 1.2020\n",
      "Epoch 75/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9799 - loss: 0.0666 - val_accuracy: 0.7100 - val_loss: 1.1720\n",
      "Epoch 76/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9806 - loss: 0.0647 - val_accuracy: 0.7038 - val_loss: 1.1695\n",
      "Epoch 77/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9757 - loss: 0.0675 - val_accuracy: 0.7250 - val_loss: 1.1293\n",
      "Epoch 78/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9776 - loss: 0.0665 - val_accuracy: 0.7138 - val_loss: 1.0725\n",
      "Epoch 79/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9745 - loss: 0.0814 - val_accuracy: 0.6963 - val_loss: 1.1630\n",
      "Epoch 80/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9801 - loss: 0.0588 - val_accuracy: 0.7075 - val_loss: 1.1796\n",
      "Epoch 81/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9789 - loss: 0.0647 - val_accuracy: 0.7025 - val_loss: 1.2116\n",
      "Epoch 82/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9764 - loss: 0.0624 - val_accuracy: 0.6913 - val_loss: 1.1776\n",
      "Epoch 83/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9794 - loss: 0.0635 - val_accuracy: 0.6975 - val_loss: 1.2620\n",
      "Epoch 84/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9781 - loss: 0.0582 - val_accuracy: 0.6875 - val_loss: 1.2519\n",
      "Epoch 85/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9823 - loss: 0.0551 - val_accuracy: 0.7038 - val_loss: 1.2086\n",
      "Epoch 86/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9778 - loss: 0.0625 - val_accuracy: 0.6913 - val_loss: 1.2106\n",
      "Epoch 87/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9774 - loss: 0.0579 - val_accuracy: 0.6787 - val_loss: 1.2663\n",
      "Epoch 88/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9786 - loss: 0.0572 - val_accuracy: 0.7038 - val_loss: 1.2585\n",
      "Epoch 89/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9857 - loss: 0.0441 - val_accuracy: 0.7013 - val_loss: 1.2886\n",
      "Epoch 90/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9764 - loss: 0.0622 - val_accuracy: 0.6975 - val_loss: 1.2617\n",
      "Epoch 91/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9855 - loss: 0.0485 - val_accuracy: 0.6950 - val_loss: 1.2891\n",
      "Epoch 92/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9825 - loss: 0.0540 - val_accuracy: 0.6950 - val_loss: 1.3193\n",
      "Epoch 93/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9800 - loss: 0.0551 - val_accuracy: 0.7013 - val_loss: 1.1917\n",
      "Epoch 94/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9826 - loss: 0.0576 - val_accuracy: 0.7038 - val_loss: 1.2508\n",
      "Epoch 95/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9800 - loss: 0.0595 - val_accuracy: 0.6950 - val_loss: 1.3350\n",
      "Epoch 96/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9774 - loss: 0.0672 - val_accuracy: 0.7100 - val_loss: 1.2845\n",
      "Epoch 97/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9872 - loss: 0.0385 - val_accuracy: 0.6975 - val_loss: 1.3011\n",
      "Epoch 98/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9799 - loss: 0.0561 - val_accuracy: 0.7000 - val_loss: 1.3466\n",
      "Epoch 99/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9866 - loss: 0.0420 - val_accuracy: 0.6862 - val_loss: 1.2835\n",
      "Epoch 100/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9822 - loss: 0.0501 - val_accuracy: 0.7000 - val_loss: 1.2829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19831660ee0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_arabert.fit(X_train_arabert_reshaped, y_train_encoded_arabert, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef4c65",
   "metadata": {},
   "source": [
    "Evaluating\n",
    "\n",
    "Bag-of-Words (BoW) Representation\n",
    "\n",
    "Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9d8b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bow_ann = ann_model_bow.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bbda631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words (BoW) Representation:\n",
      "Accuracy: 0.691\n",
      "Precision: 0.70020964360587\n",
      "Recall: 0.668\n",
      "F1-score: 0.6837256908904811\n"
     ]
    }
   ],
   "source": [
    "accuracy_bow_ann = accuracy_score(y_test_encoded_bow, y_pred_bow_ann)\n",
    "precision_bow_ann = precision_score(y_test_encoded_bow, y_pred_bow_ann)\n",
    "recall_bow_ann = recall_score(y_test_encoded_bow, y_pred_bow_ann)\n",
    "f1_bow_ann = f1_score(y_test_encoded_bow, y_pred_bow_ann)\n",
    "\n",
    "print(\"Bag-of-Words (BoW) Representation:\")\n",
    "print(\"Accuracy:\", accuracy_bow_ann)\n",
    "print(\"Precision:\", precision_bow_ann)\n",
    "print(\"Recall:\", recall_bow_ann)\n",
    "print(\"F1-score:\", f1_bow_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73fd40b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Bag-of-Words (BoW) Representation:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70       500\n",
      "           1       0.70      0.67      0.68       500\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.69      0.69      0.69      1000\n",
      "weighted avg       0.69      0.69      0.69      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report for BoW representation\n",
    "classification_report_bow_ann = classification_report(y_test_encoded_bow, y_pred_bow_ann)\n",
    "print(\"Classification Report for Bag-of-Words (BoW) Representation:\\n\", classification_report_bow_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd0b81",
   "metadata": {},
   "source": [
    "LSTM model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fad76596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data for LSTM model\n",
    "X_test_bow_reshaped = np.expand_dims(X_test_bow, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "791c7d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_bow_lstm = lstm_model_bow.predict(X_test_bow_reshaped).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4446891a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "002b5b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bow_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24b99b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words (BoW) Representation:\n",
      "Accuracy: 0.688\n",
      "Precision: 0.7259615384615384\n",
      "Recall: 0.604\n",
      "F1-score: 0.6593886462882096\n"
     ]
    }
   ],
   "source": [
    "accuracy_bow_lstm = accuracy_score(y_test_encoded_bow, y_pred_bow_lstm)\n",
    "precision_bow_lstm = precision_score(y_test_encoded_bow, y_pred_bow_lstm)\n",
    "recall_bow_lstm = recall_score(y_test_encoded_bow, y_pred_bow_lstm)\n",
    "f1_bow_lstm = f1_score(y_test_encoded_bow, y_pred_bow_lstm)\n",
    "\n",
    "print(\"Bag-of-Words (BoW) Representation:\")\n",
    "print(\"Accuracy:\", accuracy_bow_lstm)\n",
    "print(\"Precision:\", precision_bow_lstm)\n",
    "print(\"Recall:\", recall_bow_lstm)\n",
    "print(\"F1-score:\", f1_bow_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "829d2acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Bag-of-Words (BoW) Representation:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71       500\n",
      "           1       0.73      0.60      0.66       500\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.69      0.69      0.69      1000\n",
      "weighted avg       0.69      0.69      0.69      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report for BoW representation\n",
    "classification_report_bow_lstm = classification_report(y_test_encoded_bow, y_pred_bow_lstm)\n",
    "print(\"Classification Report for Bag-of-Words (BoW) Representation:\\n\", classification_report_bow_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a0109",
   "metadata": {},
   "source": [
    "AraBERT Representation\n",
    "\n",
    "Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfa1ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arabert_ann = ann_model_arabert.predict(X_test_arabert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cda54f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AraBERT Representation:\n",
      "Accuracy: 0.689\n",
      "Precision: 0.702355460385439\n",
      "Recall: 0.656\n",
      "F1-score: 0.6783867631851085\n"
     ]
    }
   ],
   "source": [
    "accuracy_arabert_ann = accuracy_score(y_test_encoded_arabert, y_pred_arabert_ann)\n",
    "precision_arabert_ann = precision_score(y_test_encoded_arabert, y_pred_arabert_ann)\n",
    "recall_arabert_ann = recall_score(y_test_encoded_arabert, y_pred_arabert_ann)\n",
    "f1_arabert_ann = f1_score(y_test_encoded_arabert, y_pred_arabert_ann)\n",
    "\n",
    "print(\"AraBERT Representation:\")\n",
    "print(\"Accuracy:\", accuracy_arabert_ann)\n",
    "print(\"Precision:\", precision_arabert_ann)\n",
    "print(\"Recall:\", recall_arabert_ann)\n",
    "print(\"F1-score:\", f1_arabert_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22672f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for AraBERT Representation:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70       500\n",
      "           1       0.70      0.66      0.68       500\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.69      0.69      0.69      1000\n",
      "weighted avg       0.69      0.69      0.69      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report for AraBERT representation\n",
    "classification_report_arabert_ann = classification_report(y_test_encoded_arabert, y_pred_arabert_ann)\n",
    "print(\"Classification Report for AraBERT Representation:\\n\", classification_report_arabert_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8fad7",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25bbcef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 768)\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train_arabert is a list\n",
    "X_test_arabert = np.array(X_test_arabert)  # Convert to NumPy array\n",
    "\n",
    "# Now you can access its shape attribute\n",
    "print(X_train_arabert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6afda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data for LSTM model\n",
    "X_test_arabert_reshaped = np.expand_dims(X_test_arabert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "345d978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_arabert_lstm = lstm_model_arabert.predict(X_test_arabert_reshaped).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78f6e3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabert Representation:\n",
      "Accuracy: 0.689\n",
      "Precision: 0.6901408450704225\n",
      "Recall: 0.686\n",
      "F1-score: 0.6880641925777332\n"
     ]
    }
   ],
   "source": [
    "accuracy_arabert_lstm = accuracy_score(y_test_encoded_arabert, y_pred_arabert_lstm)\n",
    "precision_arabert_lstm = precision_score(y_test_encoded_arabert, y_pred_arabert_lstm)\n",
    "recall_arabert_lstm = recall_score(y_test_encoded_arabert, y_pred_arabert_lstm)\n",
    "f1_arabert_lstm = f1_score(y_test_encoded_arabert, y_pred_arabert_lstm)\n",
    "\n",
    "print(\"Arabert Representation:\")\n",
    "print(\"Accuracy:\", accuracy_arabert_lstm)\n",
    "print(\"Precision:\", precision_arabert_lstm)\n",
    "print(\"Recall:\", recall_arabert_lstm)\n",
    "print(\"F1-score:\", f1_arabert_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df2c1f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Arabert Representation:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       500\n",
      "           1       0.69      0.69      0.69       500\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.69      0.69      0.69      1000\n",
      "weighted avg       0.69      0.69      0.69      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report for arabert representation\n",
    "classification_report_arabert_lstm = classification_report(y_test_encoded_arabert, y_pred_arabert_lstm)\n",
    "print(\"Classification Report for Arabert Representation:\\n\", classification_report_arabert_lstm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
